{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TPM034A Machine Learning for socio-technical systems \n",
    "## `Assignment 02: Artificial Neural Networks`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**Q2 2022**<br>\n",
    "**Instructor:** Sander van Cranenburgh <br>\n",
    "**TAs:**  Francisco Garrido Valenzuela & Lucas Spierenburg <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### `Instructions`\n",
    "\n",
    "**Assignments aim to:**<br>\n",
    "* Examine your understanding of the key concepts and techniques.\n",
    "* Examine your the applied ML skills.\n",
    "\n",
    "**Assignments:**<br>\n",
    "* Are graded and must be submitted (see the submission instruction below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### `Workspace set-up`\n",
    "**Option 1: Google Colab**<br>\n",
    "Uncomment the following cells code lines if you are running this notebook on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/TPM34A/Q2_2022\n",
    "#!pip install -r Q2_2022/requirements_colab.txt\n",
    "#!mv \"/content/Q2_2022/Assignments/assignment_02/data\" /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Option 2: Local environment**<br>\n",
    "Uncomment the following cell if you are running this notebook on your local environment. This will install all dependencies on your Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## `Application: Predicting the effects of a car ban in the city centre of Leeds` <br>\n",
    "\n",
    "### **Introduction**\n",
    "The city of Leeds, in the United Kingdom, is considering implementing a ban on private cars in the city center. Nowadays, car-free city centres are increasingly popular in Western European countries. As cars produce various externalities, including traffic accidents, air pollution, and noise pollution, a car ban has the potential to make the city centre more attractive and a better place to live and do business.\n",
    "\n",
    "Your assignment is to inform the decision-makers in Leeds about the effects of a car ban. Specifically, the city of Leeds does not yet know the extent to which a car ban would shift the mode shares of trips going to the city centre. This information is vital to assess the viability and effectiveness of the car ban policy under consideration.\n",
    "\n",
    "To inform the decision-makers in Leeds, in this assignment you will:\n",
    "1. Create a model that predicts the mode choices, given a set of travel characteristics. Specifically, you will train a neural network based on observed travel patterns. \n",
    "2. Use your trained model to predict the effect of the car ban policy on mode shares for trips going to the city centre.<br>\n",
    "\n",
    "### **Data**\n",
    "\n",
    "You have access to three data sets:\n",
    "1. Travel patterns and modes choice data. These data are obtained from a so-called revealed-preference survey, see a description of this data [here](https://link.springer.com/article/10.1007/s11116-018-9858-7)\n",
    "1. Zones of Leeds (GIS)\n",
    "1. Mode shares per zone in Leeds, derived from the two other datasets.\n",
    "<br>\n",
    "\n",
    "`IMPORTANT`<br>\n",
    "These data are exclusively made available by its owners for **educational purposes**.<br> \n",
    "You are **NOT** allowed to **share or further distribute** these data with anyone other than those involved in TPM034A.\n",
    "\n",
    "### **Notes**\n",
    "- The description of each column of revealed-preference dataset is [here](data/model_average_RP_description.pdf)\n",
    "- In revealed-preference dataset considers as *numerical travel features*: 'avail_car', 'avail_taxi', 'avail_bus' 'avail_rail', 'avail_cycling', 'avail_walking', 'total_car_cost', 'taxi_cost', 'bus_cost_total_per_leg', 'rail_cost_total_per_leg', 'car_distance_km', 'bus_distance_km', 'rail_distance_km', 'taxi_distance_km' 'cycling_distance_km', 'walking_distance_km', 'car_travel_time_min', 'bus_travel_time_min', 'rail_travel_time_min', 'taxi_travel_time_min', 'cycling_travel_time_min', 'walking_travel_time_min' 'bus_IVT_time_min', 'bus_access_egress_time_min', 'rail_IVT_time_min', 'rail_access_egress_time_min' 'bus_transfers', 'rail_transfers'.\n",
    "- Each row in the zone dataset (2nd dataset) corresponds to an individual zone in Leeds, and contains 4 different columns. The description of each column is shown in the following able:\n",
    "\n",
    "\n",
    "| Column   | Description                                                                                                                                                                                                  |\n",
    "|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| LSOA11CD | Zone Code                                                                                                                                                                                                    |\n",
    "| LSOA11NM | Zone Name                                                                                                                                                                                                    |\n",
    "| Region   | Region Code, corresponds to a bigger region formed by a set of zones. Values = {'C': Center region, 'R': Ring center region, 'NW': North-West region, 'NE': North-East , 'SW': South-West, 'SE': South-East}  |\n",
    "| geometry | Polygonal geometry of each zone                                                                                                                                                                              |\n",
    "\n",
    "\n",
    "### **Tasks and grading**\n",
    "\n",
    "Your assignment is divided into 4 subtasks: (1) Data preparation, (2) Data exploration, (3) Model training, and (4) Assessment of the impact of the car ban policy on mode shares. In total, 10 points can be earned in this assignment. The weight per subtask is shown below. \n",
    "\n",
    "1.  **Data preparation: Load datasets and make a first inspection** [1 pnt]\n",
    "    1. Load the two dataset using Pandas and GeoPandas.\n",
    "    1. Check the structure of both datasets e.g. using `df.head()` or `df.describe()`.\n",
    "    1. Handle the NaN values. I.e. only keep only trips where the **destination** is known.\n",
    "    1. Create a map that shows the six regions of Leeds (C, R, NW, NE, SW, SE) in separate colors.\n",
    "1. **Data exploration: discover and visualise pattern of mobility data.** [3 pnt] \n",
    "    1. For each zone, count the number of times that zone is a destination (hint: use the pandas *groupby* method). Create a visualisation showing the statistical distribution of these counts, using a histogram. What can you say about this distribution? \n",
    "    1. Create a visualisation showing the spatial distribution of these counts. To do so, merge this count dataframe with the geographic delination of zones.\n",
    "    1. Create a figure with 2 subplots showing the mode share of 'Car' (left) and of 'Bus' (right) in every destination zone in regions R and C, and interpret the results.<br> For your convenience, we have preprocessed the data for you. That is, we have added mode shares per destination zone. (Use the same color scale for the two maps)\n",
    "1. **Model training: Train a MultiLayerPerceptron (MLP) neural network to predict the choices** [3 pnt]\n",
    "    1. Use the *numerical travel features* (see notes above) and the following two categorical features: purpose and destination regions. Remember: (1) to scale all variables appropriately before training your MLP, and (2) to encode categorical variables.  (hint: use the pandas **get_dummies** method to encode the categorical variables).\n",
    "    1. Tune the hyperparameters of your MLP. That is, do a gridsearch over the following hyperparameter space:\n",
    "        - Architecture: {1 HL w/30 nodes, 2 HL w/ 5 nodes}\n",
    "        - Alpha parameter: {0.1, 0.001}\n",
    "        - Learning rate: {0.01, 0.001}\n",
    "    1. Fit a MLP model, using the optimal hyperparameters found and report and interpret the following output metrics:\n",
    "        - accuracy\n",
    "        - cross-entropy\n",
    "        - confusion matrix.\n",
    "1. **Assess the impact of a car ban policy on mode shares** [3 pnt]\n",
    "    1. Benchmark scenario: Create a new dataframe containing only trips with a destination in region C. Predict the mode shares for these trips, using your trained model. Use the *predict_proba* function from sk-learn, why should you NOT use the *predict* function in this case?\n",
    "    1. Car-ban scenario: In the dataset created in 4.1 set *avail_car* to zero. Use your trained model to predict the modes. (** Remember to scale the data with the scaler created for training the model**).\n",
    "    1. Compare your results. That is, analyse how mode shares have changed as a result of the car ban policy.  Create a visualisation representing the shift in mode shares. By which mode have car trips most often been substituted?\n",
    "    1. Reflect on your analysis. Do you think your analysis are meaningful? Why/why not? What is the main limitation of your analysis?\n",
    "\n",
    "\n",
    "### **Submission**\n",
    "- The deadline for this assignment is **Wed, 30 November 2022** \n",
    "- Use **Python 3.7 or above**\n",
    "- You have to submit your work in zip file with the ipynb **(fully executed)** in Brightspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import required Python packages and modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Import selected functions and classes from Python packages\n",
    "from os import getcwd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, log_loss, matthews_corrcoef, make_scorer, classification_report\n",
    "\n",
    "# Setting\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Data preparation: Load datasets and make a first inspection [1 pnt]\n",
    "#### 1.1. Load the two dataset using Pandas and GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the data folder path\n",
    "data_folder = Path(f'data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the revealed-preference dataset as pandas DataFrame\n",
    "rp_df = pd.read_csv(data_folder/'RP_mode_choice_data.csv')\n",
    "\n",
    "# Load the Leeds zones dataset as geopandas GeoDataFrame\n",
    "leeds_zones_gdf = gpd.read_file(data_folder/'Leeds_zones.gpkg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2. Check the structure of both datasets e.g. using `df.head()` or `df.describe()`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rp_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rp_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "leeds_zones_gdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "leeds_zones_gdf.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3. Handle the NaN values. I.e. only keep trips where the **destination** is known."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rp_df[rp_df.columns[rp_df.isnull().any()]].isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "leeds_zones_gdf.isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop all NaN values in the destination column in the revealed-preference dataset as they cannot be used for destination predictions\n",
    "rp_df = rp_df.dropna(subset=['Destination_lsoa_code'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rp_df[rp_df.columns[rp_df.isnull().any()]].isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.4. Create a map that shows the six regions of Leeds (C, R, NW, NE, SW, SE) in separate colors.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw the map based on region\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "leeds_zones_gdf.plot(ax=ax, column = 'Region', legend = True, cmap='viridis')\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\"Leeds zones grouped as regions\")\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Data exploration: discover and visualise mobility patterns. [3 pnt]\n",
    "#### 2.1 For each zone, count the number of times that zone is a destination (hint: use the pandas *groupby* method). Create a visualisation showing the statistical distribution of these counts, using a histogram. What can you say about this distribution?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "zone_destination_count = rp_df['Destination_lsoa_code'].value_counts()\n",
    "zone_destination_count.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create histogram and empirical CDF for zone destination count\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5), sharex=True)\n",
    "sns.histplot(ax = axes[0],x = zone_destination_count)\n",
    "ecdf_data = sns.ecdfplot(ax = axes[1],x = zone_destination_count)\n",
    "axes[0].set_xlabel(\"Zone ID\")\n",
    "axes[1].set_xlabel(\"Zone ID\")\n",
    "axes[1].grid(True,linewidth = 0.5)\n",
    "axes[1].minorticks_on()\n",
    "axes[1].grid(which='minor', linestyle=':', linewidth='0.5', color='black')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The amount of times a zone is a destination is heavily skewed towards a very small number of popular destination zones. Most others zones are very little visited as destinations. This will probably have to do with zones where people go for work, study, shopping or leisure as opposed to zones which are \"only\" for living."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2 Create a visualisation showing the *spatial* distribution of these counts. To do so, merge this count dataframe with the geographic delination of zones."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a mergeable DataFrame from the count series\n",
    "zone_destination_count_df = zone_destination_count.to_frame()\n",
    "zone_destination_count_df.reset_index(level=0, inplace=True)\n",
    "zone_destination_count_df.rename(columns = {'index':'LSOA11CD', 'Destination_lsoa_code':'zone_destination_count'}, inplace=True)\n",
    "zone_destination_count_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Merge the zone destination count DataFrame with the GeoDataFrame\n",
    "leeds_zones_gdf = leeds_zones_gdf.merge(zone_destination_count_df, on=\"LSOA11CD\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot the map\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "leeds_zones_gdf.plot(ax=ax, column = 'zone_destination_count', legend = True, cmap='viridis')\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\"Map of Leeds zones with destination count\")\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3. Create a figure with 2 subplots showing the mode share of 'Car' (left) and of 'Bus' (right) in every destination zone in regions R and C, and interpret the results.<br> For your convenience, we have preprocessed the data for you. That is, we have added mode shares per destination zone. (Use the same color scale for the two maps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the Leeds mode share per zone dataset as geopandas GeoDataFrame\n",
    "mode_shares_per_zone_gdf = gpd.read_file(data_folder/'mode_shares_per_zones.gpkg')\n",
    "mode_shares_per_zone_gdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create spatial maps showing the buurten of Amsterdam with real estate prices and liveability levels for both 2014 and 2020\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), sharex=True, sharey=True)\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "mode_shares_per_zone_gdf[(mode_shares_per_zone_gdf.Region == 'C') | (mode_shares_per_zone_gdf.Region == 'R')].plot(ax=axes[0], column = 'car', legend=False, cmap='viridis')\n",
    "mode_shares_per_zone_gdf[(mode_shares_per_zone_gdf.Region == 'C') | (mode_shares_per_zone_gdf.Region == 'R')].plot(ax=axes[1], column = 'bus', legend=True, cmap='viridis')\n",
    "\n",
    "axes[0].set_title(\"Car share for zones in regions C and R in Leeds\")\n",
    "axes[0].axis('off')\n",
    "axes[1].set_title(\"Bus share for zones in regions C and R in Leeds\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw the map based on region\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "leeds_zones_gdf[(leeds_zones_gdf.Region == 'C') | (leeds_zones_gdf.Region == 'R')].plot(ax=ax, column = 'Region', legend = True, cmap='viridis')\n",
    "ax.set_axis_off()\n",
    "ax.set_title(\"Leeds zones in regions C and R\")\n",
    "plt.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In general, car use is the dominant share throughout these regions. It seems that the zones in the North-East area of the regions C and R are relatively more popular to visit by car than the other zones. Therefore, bus share is low in this same area. For the bus share, it seems that only a few zones are heavily visited by bus whereas others seem to show quite a low average bus share. In addition, it does not seem that the zones in the center region show very different behaviour from the zones in the ring region, which might have been expected based on average city policies for visiting (old) city centres."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Model training: Train a MultiLayerPerceptron (MLP) neural network to predict the choices [3 pnt]\n",
    "\n",
    "#### 3.1. Use the *numerical travel features* (see notes above) and the following two categorical features: purpose and destination regions. Remember: (1) to scale all variables appropriately before training your MLP, and (2) to encode categorical variables.  (hint: use the pandas **get_dummies** method to encode the categorical variables)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "zone_region_df = leeds_zones_gdf[['LSOA11CD', 'Region']]\n",
    "zone_region_df = zone_region_df.rename(columns={\"LSOA11CD\": \"Destination_lsoa_code\"})\n",
    "\n",
    "rp_df = rp_df.merge(zone_region_df, on='Destination_lsoa_code')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rp_df = pd.get_dummies(rp_df, columns=['purpose', 'Region'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the list of features that we want to use in the model\n",
    "features = ['avail_car', 'avail_taxi', 'avail_bus', 'avail_rail', 'avail_cycling', 'avail_walking', 'total_car_cost', 'taxi_cost', 'bus_cost_total_per_leg',\n",
    "            'rail_cost_total_per_leg', 'car_distance_km', 'bus_distance_km', 'rail_distance_km', 'taxi_distance_km', 'cycling_distance_km', 'walking_distance_km',\n",
    "            'car_travel_time_min', 'bus_travel_time_min', 'rail_travel_time_min', 'taxi_travel_time_min', 'cycling_travel_time_min', 'walking_travel_time_min',\n",
    "            'bus_IVT_time_min', 'bus_access_egress_time_min', 'rail_IVT_time_min', 'rail_access_egress_time_min', 'bus_transfers', 'rail_transfers',\n",
    "            'purpose_Cinema or other night out', 'purpose_Clothes shopping', 'purpose_College/University', 'purpose_Dropoff Daycare', 'purpose_Dropoff K12',\n",
    "            'purpose_Dropoff Other', 'purpose_Dropoff Scheduled Activity', 'purpose_Dropoff Work', 'purpose_Errand Other', 'purpose_Errands with Appointment',\n",
    "            'purpose_Errands without Appointment', 'purpose_Exercise', 'purpose_Family Activity', 'purpose_Gas', 'purpose_Grocery', 'purpose_Home', 'purpose_K-12 School',\n",
    "            'purpose_Leisure Other', 'purpose_Medical', 'purpose_Museum/cultural', 'purpose_OtherPurpose', 'purpose_Primary Workplace', 'purpose_Restaurant',\n",
    "            'purpose_Shopping - Major', 'purpose_Social', 'purpose_Sports activity', 'purpose_Vacation/Travel', 'purpose_Vocational education', 'purpose_Work Other',\n",
    "            'purpose_Work Related', 'purpose_Work Travel', 'purpose_Work Volunteer', 'Region_C', 'Region_NE', 'Region_NW', 'Region_R', 'Region_SE', 'Region_SW']\n",
    "\n",
    "X = rp_df.loc[:,features]\n",
    "\n",
    "# Initiate scaler object & fit to data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "\n",
    "# Create new dataframe X_scaled containing the scaled features\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Create the target\n",
    "Y = rp_df['choice']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 Tune the hyperparameters of your MLP. That is, do a gridsearch over the following hyperparameter space:\n",
    "        - Architecture: {1 HL w/30 nodes, 2 HL w/ 5 nodes}\n",
    "        - Alpha parameter: {0.1, 0.001}\n",
    "        - Learning rate: {0.01, 0.001}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create MLP object (plain vanilla MLP)\n",
    "mlp_gs = MLPClassifier(activation = 'tanh', solver='adam', batch_size=250, max_iter=2000)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "hyperparameter_space = {\n",
    "    'hidden_layer_sizes': [(30),(5,5)],\n",
    "    'alpha': [0.1, 0.001],\n",
    "    'learning_rate_init': [0.01,0.001]}\n",
    "\n",
    "# Create scoring function\n",
    "logloss = make_scorer(log_loss, greater_is_better = False, needs_proba = True)\n",
    "\n",
    "# Create the grid_search object, with using the MLP classifier\n",
    "folds = 5 # Number of cross validation splits\n",
    "mlp_gridsearch = GridSearchCV(mlp_gs, hyperparameter_space, n_jobs=-1, cv=folds,scoring = logloss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, random_state = 12345, test_size = 0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Execute the training/gridsearch\n",
    "mlp_gridsearch.fit(X_train, Y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save your model\n",
    "filename = 'my_tuned_model.sav'\n",
    "pickle.dump(mlp_gridsearch, open(data_folder/filename,'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3 Fit a MLP model, using the optimal hyperparameters found and report and interpret the following output metrics:\n",
    "        - Accuracy\n",
    "        - Cross-entropy\n",
    "        - Confusion matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a new mlp object using the optimised hyperparameters, just using the train/test split\n",
    "layers = mlp_gridsearch.best_params_['hidden_layer_sizes']\n",
    "lr = mlp_gridsearch.best_params_['learning_rate_init']\n",
    "alpha = mlp_gridsearch.best_params_['alpha']\n",
    "mlp_gs = MLPClassifier(hidden_layer_sizes = layers, solver='adam', learning_rate_init = lr, alpha=alpha, batch_size=250, activation = 'tanh', max_iter = 2000)\n",
    "\n",
    "# Train the model\n",
    "mlp_gs.fit(X_train,Y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's create a function that returns the accuracy and the cross entropy, for the train and test data sets\n",
    "def calculate_acc_ce(mlp,X_train,Y_train,X_test, Y_test):\n",
    "\n",
    "    def calculate_acc(mlp,X,Y):\n",
    "        accuracy = mlp.score(X,Y)\n",
    "        return accuracy\n",
    "\n",
    "    def calculate_ce(mlp,X,Y):\n",
    "        # Compute cross entropy\n",
    "        # Use the model object to predict probabilities per class\n",
    "        prob = mlp.predict_proba(X)\n",
    "\n",
    "        # Multiply the probabilities with Y (0/1 array), and sum along the row axis to obtain the predicted probability of the target\n",
    "        Y_dummy = pd.get_dummies(Y).to_numpy()\n",
    "        prob_chosen = np.sum(prob*Y_dummy,axis=1)\n",
    "\n",
    "        # Take the logarithm\n",
    "        log_prob_chosen = np.log(prob_chosen)\n",
    "\n",
    "        # Compute the cross entropy\n",
    "        cross_entropy = -np.sum(log_prob_chosen)/len(Y)\n",
    "        return cross_entropy\n",
    "\n",
    "    # Compute the accuracy\n",
    "    acc_train = calculate_acc(mlp,X_train,Y_train)\n",
    "    acc_test  = calculate_acc(mlp,X_test,Y_test)\n",
    "\n",
    "    # Apply cross entropy function\n",
    "    ce_train = calculate_ce(mlp,X_train,Y_train)\n",
    "    ce_test = calculate_ce(mlp,X_test,Y_test)\n",
    "    return acc_train, acc_test, ce_train, ce_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's also evaluate performance of the hypertuned model using our evaluation function\n",
    "accuracy_train_gs, accuracy_test_gs, cross_entropy_train_gs, cross_entropy_test_gs = calculate_acc_ce(mlp_gs,X_train,Y_train,X_test, Y_test)\n",
    "\n",
    "# Report results\n",
    "print('\\t\\t Train set\\t Test    set')\n",
    "print(f'Accuracy\\t {accuracy_train_gs:0.3f}\\t\\t {accuracy_test_gs:0.3f}')\n",
    "print(f'Cross entropy\\t {cross_entropy_train_gs:0.3f}\\t\\t {cross_entropy_test_gs:0.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_pred_gs = mlp_gs.predict(X_test)\n",
    "\n",
    "# Show the confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize = (20,10))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "ylabels = ['Car', 'Bus', 'Rail', 'Taxi', 'Cycling', 'Walking']\n",
    "cm1 = ConfusionMatrixDisplay.from_predictions(ax=axes[0], y_true=Y_test,y_pred=Y_pred_gs, display_labels = ylabels, normalize=None)\n",
    "cm2 = ConfusionMatrixDisplay.from_predictions(ax=axes[1], y_true=Y_test,y_pred=Y_pred_gs, display_labels = ylabels, normalize='true')\n",
    "\n",
    "# Add titles\n",
    "axes[0].set_title(f'MLP with {mlp_gs.hidden_layer_sizes} nodes')\n",
    "axes[1].set_title(f'MLP with {mlp_gs.hidden_layer_sizes} nodes')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the accuracy of 0.854 on the test set, the model seems to perform generally well. In addition, the confusion matrix shows strong rightfully predictions for the diagonal. The most significant errors occur for the model predicting bus, while in reality it was actually taxi or cycling. I assume this is caused by for people often dynamically switching between those three options when having to make a similar choice."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.1. Benchmark scenario: create a new dataframe containing only trips with a destination in region C. Predict the mode shares for these trips, using your trained model. Use the *predict_proba* function from sk-learn (why should you NOT use the *predict* function in this case?)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_regionC = X[X['Region_C']==1]\n",
    "X_regionC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_scaled_regionC = scaler.transform(X_regionC)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_pred_gs_regionC = mlp_gs.predict_proba(X_scaled_regionC)\n",
    "Y_pred_gs_regionC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should not use predict as opposed to predict_proba as the first reduces a lot of insights when compared with the latter because it provides insights in the distribution and (un)certainty of a certain prediction, which information you would lose when using predict to obtain a single class value prediction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2. Car-ban scenario: in the dataset created in 4.1 set *avail_car* to zero. Use your trained model to predict the modes. (**Remember to scale the data with the scaler created for training the model**).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_regionC_availcar0 = X_regionC\n",
    "X_regionC_availcar0['avail_car'].values[:] = 0\n",
    "X_regionC_availcar0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_scaled_regionC_availcar0 = scaler.transform(X_regionC_availcar0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_pred_gs_regionC_availcar0 = mlp_gs.predict_proba(X_scaled_regionC_availcar0)\n",
    "Y_pred_gs_regionC_availcar0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.3. Compare your results. That is, analyse how mode shares have changed as a result of the car-ban policy. Create a visualisation representing the shift in mode shares. By which mode have car trips most often been substituted?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "average_mode_Y_pred_gs_regionC = np.mean(Y_pred_gs_regionC, axis=0)\n",
    "average_mode_Y_pred_gs_regionC_availcar0 = np.mean(Y_pred_gs_regionC_availcar0, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "average_mode_change = abs(average_mode_Y_pred_gs_regionC_availcar0 - average_mode_Y_pred_gs_regionC)\n",
    "average_mode_change_without_car = np.delete(average_mode_change, 0)\n",
    "average_mode_change_without_car"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = ['Bus', 'Rail', 'Taxi', 'Cycling', 'Walking']\n",
    "\n",
    "plt.pie(average_mode_change_without_car, labels=labels)\n",
    "plt.show"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is clearly visible that a car ban has resulted in the biggest substitution of cars by bus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.3.Reflect on your analysis: <br> \n",
    "`A` Do you think your analysis and results are meaningful? Why/why not? <br> \n",
    "`B` What are the main limitations of your analysis?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Yes, I think the analysis and resluts are meaningful as there was enough balanced (real captured) data available for the model to learn the \"normal\" situation and be able to predict the consequences of policy changes.\n",
    "\n",
    "The biggest limitation is that a full car ban in region C would have people probably rethink their transport options again, which is now not included in the dataset and therefore the neural network model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit ('3.9.15')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a51f908579e300a02f10534cef0cd09f6905b9ea3f83df74d0f938cc2c2730e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}